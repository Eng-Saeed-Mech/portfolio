import requests
from bs4 import BeautifulSoup
import csv
import datetime

# Ù†Ø³ØªØ®Ø¯Ù… HTTPS ÙˆÙ…Ø­Ø±Ùƒ LXML Ø§Ù„Ø£Ù‚ÙˆÙ‰
url = "https://books.toscrape.com/"

# Ø¨Ù†Ø¹Ø±Ù Ù†ÙØ³Ù†Ø§ ÙƒÙ…ØªØµÙØ­ Ø¹Ø´Ø§Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…ÙŠØ¹Ù…Ù„Ø´ Ø¨Ù„ÙˆÙƒ
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

filename = "products_data.csv"
header = ['Product Name', 'Price', 'Availability', 'Date Scraped']

print(f"ğŸ”„ Connecting to {url}...")

try:
    # Ø¨Ù†Ø¨Ø¹Øª Ø§Ù„Ù‡ÙŠØ¯Ø± (Ø§Ù„Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ø®ØµÙŠØ©) Ù…Ø¹ Ø§Ù„Ø·Ù„Ø¨
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… lxml Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† html.parser Ù„Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰
        soup = BeautifulSoup(response.text, 'lxml')
        
        books = soup.find_all('article', class_='product_pod')
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            
            count = 0
            for book in books:
                title = book.h3.a['title']
                price = book.find('p', class_='price_color').text
                availability = book.find('p', class_='instock availability').text.strip()
                today = datetime.date.today()
                
                writer.writerow([title, price, availability, today])
                count += 1
                
        if count > 0:
            print(f"âœ… Success! Scraped {count} items.")
            print(f"ğŸ“‚ Data saved to '{filename}' inside the folder.")
        else:
            print("âš ï¸ Connected, but found 0 items. Check the parsers.")
            
    else:
        print("âŒ Failed to retrieve the webpage.")

except Exception as e:
    print(f"âŒ Error: {e}")